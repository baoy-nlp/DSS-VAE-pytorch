base_configs:
# file configs:
  start_iter: 0
  config_path: /data00/home/baoyu.nlp/toutiao-pytorch/vae-pytorch/configs/model_configs/nag-att.yaml
  train_file: /data00/home/baoyu.nlp/toutiao-pytorch/snli-ae/train.bin
  dev_file: /data00/home/baoyu.nlp/toutiao-pytorch/snli-ae/dev.bin
  test_file: /data00/home/baoyu.nlp/toutiao-pytorch/snli-ae/test.bin
  vocab: /data00/home/baoyu.nlp/toutiao-pytorch/snli-ae/vocab.bin
  logdir: &log /data00/home/baoyu.nlp/run_logs
  model_dir: /data00/home/baoyu.nlp/run_models
# model configs:
  mode: ~
# model parameter set
  cuda: &use_cuda true
  seed: 5783287
# train configs:
  batch_size: 50
  lr: 0.001
  lr_decay: 0.7
  lr_decay_after_epoch: 5
  log_every: 50
  patience: 8
  max_num_trial: 20
  dev_every: 500
  clip_grad: 5.0
  reset_optimizer: false
  src_max_time_step: &smts 55
  tgt_max_time_step: &tmts 55
  beam_size: 5
  clip_learning_signal: ~
  valid_metric: p_acc
  uniform_init: ~
  max_epoch: 20000
  kl_anneal: false
  alpha: 0.1
  verbose: false
  exp_name: &exp_n ~
  model_backup: reproduct

vae_configs:
  model_select: &vms SyntaxVAE
  dis_train: true
  embed_size: &ves 300
  hidden_size: &vhs 300
  num_layers: &vnl 2
  rnn_drop: &vrd 0.3
  latent_size: 100
# seq2seq model parameter
  rnn_type: gru
  src_max_time_step: *smts
  enc_embed_dim: *ves
  enc_hidden_dim: *vhs
  enc_num_layers: *vnl
  bidirectional: true
  mapper_type: link
  tgt_max_time_step: *tmts
  dec_embed_dim: *ves
  dec_hidden_dim: *vhs
  dec_num_layers: *vnl
# vae parameter
  cuda: *use_cuda
  decode_max_time_step: *smts
  eval_mode: bleu
  logdir: *log
  log_every: 10
  model_file: *vms
  max_sequecne_length: 300
  print_every: 10
  sample_size: 5
  share_embed: true
  tensorboard_logging: true
  use_attention: false
  enc_ed: 0.0
  enc_rd: 0.0
  dec_ed: *vrd
  dec_rd: *vrd
  unk_rate: 0.50
  k: 0.0025
  x0: 5000
  warm_up: 5000
  epochs: 10
  anneal_function: logistic
  unk_schedule: fixed
  src_wd: false
  tgt_wd: true
  peak_anneal: false
  init_step_kl_weight: ~
  stop_clip_kl: 0.3
  reload_model: false
  kl_factor: 1.0
#  aux_weight: 0.3
  mul_syn: 1.0
  mul_sem: 5.0
  adv_syn: 0.5
  adv_sem: 0.5
  infer_weight: 1.0
  inf_syn: 0.5
  inf_sem: 0.5
  syn_weight: 2.0
  sem_weight: 1.0
  eval_adv: false
  eval_mul: false
  dev_item: ELBO
  eval_bs: 50
  report: ~

ae_configs:
  embed_size: &aes 300
  hidden_size: &ahs 300
  num_layers: &anl 3
  rnn_drop: &ard 0.1
# seq2seq model parameter
  rnn_type: gru
  src_max_time_step: *smts
  enc_embed_dim: *aes
  enc_hidden_dim: *ahs
  enc_num_layers: *anl
  bidirectional: true
  use_attention: false
  mapper_type: link
  tgt_max_time_step: *tmts
  dec_embed_dim: *aes
  dec_hidden_dim: *ahs
  dec_num_layers: *anl
  enc_ed: *ard
  enc_rd: *ard
  dec_ed: *ard
  dec_rd: *ard
  model_select: &ae AutoEncoder
  model_file: *ae
  share_embed: true
  cuda: *use_cuda
  word_drop: 0.25
  sample_size: 5
  eval_mode: bleu

nag_configs:
  embed_size: &nes 300
  hidden_size: *nes
  model_select: &nae ParallelAE
  enc_type: att
  src_max_time_step: *smts
  tgt_max_time_step: *tmts
  enc_embed_dim: *nes
  enc_hidden_dim: *nes
  enc_ed: &ned 0.1
  enc_rd: &nrd 0.1
  bidirectional: True
  rnn_type: gru
  enc_head: 6
  enc_num_layers: 4
  enc_inner_hidden: 600
  dec_type: mat
  dec_num_layers: 4
  dec_head: 6
  dec_hidden_dim: *nes
  dec_inner_hidden: 600
  dec_rd: *nrd
  dropm: *ned
  dropo: *nrd
  word_drop: 0.0
  cuda: *use_cuda
  eval_mode: bleu
  eval_bs: 100
  model_file: *nae
  dev_item: BLEU
